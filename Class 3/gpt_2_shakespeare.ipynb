{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "817c90f6-60a1-409d-cde1-9dd30ff70248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.31.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (15.0.6.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.51.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.19.6)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (63.4.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24576 sha256=a2351931598ae1a83757fe258c5b64717da2feee20b1a7a2456239a7d335f8a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/28/f0/2f12e470be10d6804b193e4193d274c88995010fae512a67cf\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "acd36ba0-6d65-46aa-e77e-23b67157b040"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 265Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 5.12Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 740Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:10, 49.0Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 707Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 6.67Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 6.79Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "c5edb13b-ddac-42cc-9906-d396b2c020f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "56a8486a-6e1f-4540-dce1-ccb40e0f9bde"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth? Maybe you're on the Red Planet. Do you have any ideas for future planets?\n",
            "\n",
            "I think everyone has their own ideas. It's not a question of, \"Well, anybody could come up with a next planet,\" it's a question\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "27932fe4-fd92-470e-b0c0-fb681b3fa3a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 12:45:49--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.135.152, 52.217.174.240, 52.216.224.251, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.135.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txtâ€™\n",
            "\n",
            "nietzsche.txt       100%[===================>] 586.82K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-03-22 12:45:49 (4.16 MB/s) - â€˜nietzsche.txtâ€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "97f35e0b-0f9f-43be-adba-65db37e1359f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 203MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "c348eed8-8de0-47ec-82c6-6026867b8fce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 12:48:33--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-03-22 12:48:34 (21.1 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'got1.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "3d2da006-e342-4d8b-9b36-62746dd7601e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 433157 tokens\n",
            "Training...\n",
            "[1 | 7.50] loss=3.55 avg=3.55\n",
            "[2 | 9.89] loss=3.46 avg=3.50\n",
            "[3 | 12.30] loss=3.49 avg=3.50\n",
            "[4 | 14.75] loss=3.51 avg=3.50\n",
            "[5 | 17.23] loss=3.42 avg=3.49\n",
            "[6 | 19.73] loss=3.35 avg=3.46\n",
            "[7 | 22.22] loss=3.23 avg=3.43\n",
            "[8 | 24.69] loss=3.23 avg=3.40\n",
            "[9 | 27.13] loss=3.42 avg=3.40\n",
            "[10 | 29.54] loss=3.24 avg=3.39\n",
            "[11 | 31.93] loss=3.15 avg=3.37\n",
            "[12 | 34.29] loss=3.14 avg=3.35\n",
            "[13 | 36.64] loss=3.27 avg=3.34\n",
            "[14 | 38.97] loss=3.08 avg=3.32\n",
            "[15 | 41.28] loss=3.13 avg=3.31\n",
            "[16 | 43.59] loss=3.21 avg=3.30\n",
            "[17 | 45.90] loss=3.11 avg=3.29\n",
            "[18 | 48.19] loss=3.13 avg=3.28\n",
            "[19 | 50.47] loss=3.20 avg=3.27\n",
            "[20 | 52.75] loss=3.09 avg=3.26\n",
            "[21 | 55.02] loss=3.03 avg=3.25\n",
            "[22 | 57.28] loss=3.06 avg=3.24\n",
            "[23 | 59.55] loss=3.28 avg=3.24\n",
            "[24 | 61.83] loss=3.11 avg=3.24\n",
            "[25 | 64.11] loss=3.13 avg=3.23\n",
            "[26 | 66.38] loss=3.12 avg=3.23\n",
            "[27 | 68.65] loss=3.09 avg=3.22\n",
            "[28 | 70.93] loss=3.17 avg=3.22\n",
            "[29 | 73.21] loss=3.14 avg=3.22\n",
            "[30 | 75.49] loss=3.03 avg=3.21\n",
            "[31 | 77.78] loss=3.09 avg=3.21\n",
            "[32 | 80.07] loss=2.98 avg=3.20\n",
            "[33 | 82.37] loss=3.00 avg=3.19\n",
            "[34 | 84.68] loss=3.09 avg=3.19\n",
            "[35 | 87.00] loss=3.07 avg=3.18\n",
            "[36 | 89.32] loss=3.10 avg=3.18\n",
            "[37 | 91.64] loss=2.96 avg=3.17\n",
            "[38 | 93.97] loss=3.02 avg=3.17\n",
            "[39 | 96.30] loss=2.93 avg=3.16\n",
            "[40 | 98.65] loss=2.96 avg=3.16\n",
            "[41 | 101.00] loss=2.97 avg=3.15\n",
            "[42 | 103.33] loss=3.02 avg=3.15\n",
            "[43 | 105.67] loss=3.09 avg=3.14\n",
            "[44 | 108.01] loss=2.97 avg=3.14\n",
            "[45 | 110.36] loss=3.01 avg=3.14\n",
            "[46 | 112.70] loss=3.17 avg=3.14\n",
            "[47 | 115.03] loss=3.08 avg=3.14\n",
            "[48 | 117.37] loss=2.95 avg=3.13\n",
            "[49 | 119.70] loss=2.92 avg=3.12\n",
            "[50 | 122.04] loss=2.90 avg=3.12\n",
            "[51 | 124.38] loss=2.84 avg=3.11\n",
            "[52 | 126.71] loss=3.09 avg=3.11\n",
            "[53 | 129.03] loss=2.99 avg=3.11\n",
            "[54 | 131.34] loss=2.92 avg=3.10\n",
            "[55 | 133.65] loss=2.98 avg=3.10\n",
            "[56 | 135.97] loss=3.05 avg=3.10\n",
            "[57 | 138.29] loss=2.92 avg=3.10\n",
            "[58 | 140.59] loss=2.81 avg=3.09\n",
            "[59 | 142.90] loss=3.08 avg=3.09\n",
            "[60 | 145.19] loss=2.97 avg=3.09\n",
            "[61 | 147.49] loss=3.02 avg=3.09\n",
            "[62 | 149.80] loss=2.88 avg=3.08\n",
            "[63 | 152.10] loss=2.93 avg=3.08\n",
            "[64 | 154.39] loss=2.87 avg=3.07\n",
            "[65 | 156.69] loss=2.90 avg=3.07\n",
            "[66 | 159.00] loss=2.84 avg=3.06\n",
            "[67 | 161.32] loss=2.94 avg=3.06\n",
            "[68 | 163.62] loss=2.91 avg=3.06\n",
            "[69 | 165.92] loss=2.81 avg=3.05\n",
            "[70 | 168.23] loss=2.77 avg=3.05\n",
            "[71 | 170.53] loss=2.85 avg=3.04\n",
            "[72 | 172.84] loss=2.75 avg=3.04\n",
            "[73 | 175.15] loss=2.93 avg=3.04\n",
            "[74 | 177.47] loss=2.94 avg=3.03\n",
            "[75 | 179.78] loss=2.88 avg=3.03\n",
            "[76 | 182.09] loss=2.74 avg=3.03\n",
            "[77 | 184.41] loss=2.81 avg=3.02\n",
            "[78 | 186.73] loss=2.91 avg=3.02\n",
            "[79 | 189.04] loss=2.91 avg=3.02\n",
            "[80 | 191.36] loss=2.84 avg=3.02\n",
            "[81 | 193.68] loss=2.81 avg=3.01\n",
            "[82 | 195.99] loss=2.83 avg=3.01\n",
            "[83 | 198.32] loss=2.82 avg=3.00\n",
            "[84 | 200.64] loss=2.73 avg=3.00\n",
            "[85 | 202.96] loss=2.96 avg=3.00\n",
            "[86 | 205.28] loss=2.86 avg=3.00\n",
            "[87 | 207.60] loss=2.87 avg=2.99\n",
            "[88 | 209.92] loss=2.86 avg=2.99\n",
            "[89 | 212.24] loss=3.12 avg=2.99\n",
            "[90 | 214.57] loss=3.03 avg=3.00\n",
            "[91 | 216.89] loss=2.78 avg=2.99\n",
            "[92 | 219.21] loss=2.90 avg=2.99\n",
            "[93 | 221.54] loss=2.84 avg=2.99\n",
            "[94 | 223.87] loss=2.77 avg=2.98\n",
            "[95 | 226.18] loss=2.72 avg=2.98\n",
            "[96 | 228.49] loss=2.76 avg=2.98\n",
            "[97 | 230.81] loss=2.83 avg=2.97\n",
            "[98 | 233.13] loss=2.78 avg=2.97\n",
            "[99 | 235.44] loss=2.77 avg=2.97\n",
            "[100 | 237.76] loss=2.85 avg=2.97\n",
            "======== SAMPLE 1 ========\n",
            " Father, and the king was king, as ever. \n",
            "The knight asked if they would take us to the lake, to the island, to the castle. \n",
            "\"Yes,\" I said respectfully. \"We can keep these as you wish. They're for me. For \n",
            "you. And for him.\" \n",
            "His tone was melancholy. It was dark now. He told me he went to ask for the \n",
            "jinn, only to find that he had not sent him. When I asked if they would bring us, he told me they had not \n",
            "announced. \n",
            "I was not sure what to make of it. \n",
            "The lord had taken all three in mind. \"The king loves us here,\" he said. \"He's \n",
            "looking off at the sky.\" He had to explain himself, to some one here, to the lord. His brother had come here with \n",
            "him. He had given it no thought. We had gone to the river again, my lord, through the woods. \n",
            "\"And here the sun went down,\" I told him. \"It was as well.\" \n",
            "He frowned. \"No, no!\" he shouted. He turned round and ran over to us. \"No, \n",
            "No, no, no!\" He hugged me tight. He was so touched, he wanted to touch me, he wanted to touch me. \n",
            "My father did not trust him. \"No, no, not this time,\" he whispered. \n",
            "\"No, not this time!\" I protested. \"No, no, you have to.\" \n",
            "He hugged me hard. He whispered in my ear, too. \"No, yes, no, I cannot.\" \n",
            "He lifted my gaze to the sky. He was looking down at the sky. He had no more eyes than \n",
            "the Lord of the Rings. \n",
            "He was not looking down or waiting in the stars. He was looking back. He was not looking down. He looked down and could \n",
            "feel himself falling away, and he was not looking away. \n",
            "The knight turned on him. \"No, Father, you must not touch this man,\" he shouted. \"You must not touch him \n",
            "to the death.\" \n",
            "I felt so badly for my father now. \n",
            "He felt like a child again, I told him. \"You can kiss me, I promise you. You can kiss a \n",
            "child.\" \"Yes, I promise.\" I turned away from him. \n",
            "The man in front of me whispered something. He was a man. \"What else would we have to do?\" he \n",
            "said, with his face still as pale as snow. He looked at me strangely, and whispered to me. \"This is \n",
            "not happening any more.\" \n",
            "\"See?\" I said. \"Do you understand your words, Father?\" He gave him a look I did not understand. He was \n",
            "shaking. \n",
            "\"What are you talking about, little boy?\" I said. \n",
            "\"He says you saw, as he says.\" He went to the window. \"No, not like this in the middle of the \n",
            "world.\" He pushed open the window. The sun was gone. \n",
            "\"He says you saw,\" I said. \"He says you saw. He says he saw you look at him.\" \n",
            "His face was red with anger. Why did he say this? Why did he say this? He was angry, he \n",
            "said that, and he was trying to hide. \"How do you know it was you?\" \n",
            "\"I had no doubt in my mind,\" I told him. His face was hard with anger. What was he, \n",
            "how could a man want the help he cravens here in a king's castle, where so many children have died? \n",
            "He was angry. \n",
            "\"And what was the king thinking?\" I asked. \"He saw. Who knew what he was \n",
            "doing?\" \n",
            "His voice grew hoarse. \"The son of Ammon. I have not seen him in years.\" \n",
            "\"How do you know he didn't see?\" \n",
            "\"The king had no need of witnesses. The king believed his own.\" \n",
            "\"And so did the godson, the king loved him more than he did a brother.\" \n",
            "\"We did love you, Father. We swore an oath to you. Now it's the word of the gods.\" \n",
            "\"He said it was true,\" I told him. \"What can you tell me?\" \n",
            "His eyes narrowed again. He looked at me so badly. \"You took his life, and not me.\" He smiled. \"You had no idea, I \n",
            "know, how often I say that.\" \n",
            "The knight was cold as a man who lies. He was afraid. And angry. I had no reason to fear. I knew \n",
            "it would happen\n",
            "\n",
            "[101 | 252.51] loss=2.72 avg=2.96\n",
            "[102 | 254.82] loss=2.81 avg=2.96\n",
            "[103 | 257.14] loss=2.81 avg=2.96\n",
            "[104 | 259.47] loss=2.76 avg=2.95\n",
            "[105 | 261.78] loss=2.75 avg=2.95\n",
            "[106 | 264.09] loss=2.87 avg=2.95\n",
            "[107 | 266.40] loss=2.88 avg=2.95\n",
            "[108 | 268.72] loss=2.76 avg=2.95\n",
            "[109 | 271.03] loss=3.02 avg=2.95\n",
            "[110 | 273.35] loss=2.82 avg=2.95\n",
            "[111 | 275.66] loss=2.76 avg=2.94\n",
            "[112 | 277.97] loss=2.73 avg=2.94\n",
            "[113 | 280.28] loss=2.76 avg=2.94\n",
            "[114 | 282.59] loss=2.72 avg=2.93\n",
            "[115 | 284.91] loss=2.71 avg=2.93\n",
            "[116 | 287.23] loss=2.94 avg=2.93\n",
            "[117 | 289.54] loss=2.87 avg=2.93\n",
            "[118 | 291.85] loss=2.79 avg=2.93\n",
            "[119 | 294.18] loss=2.80 avg=2.93\n",
            "[120 | 296.49] loss=2.82 avg=2.92\n",
            "[121 | 298.81] loss=2.82 avg=2.92\n",
            "[122 | 301.12] loss=2.67 avg=2.92\n",
            "[123 | 303.44] loss=2.72 avg=2.92\n",
            "[124 | 305.76] loss=2.79 avg=2.91\n",
            "[125 | 308.07] loss=2.83 avg=2.91\n",
            "[126 | 310.38] loss=2.83 avg=2.91\n",
            "[127 | 312.69] loss=2.83 avg=2.91\n",
            "[128 | 315.00] loss=2.83 avg=2.91\n",
            "[129 | 317.33] loss=2.87 avg=2.91\n",
            "[130 | 319.65] loss=2.80 avg=2.91\n",
            "[131 | 321.96] loss=2.72 avg=2.91\n",
            "[132 | 324.27] loss=2.78 avg=2.90\n",
            "[133 | 326.59] loss=2.73 avg=2.90\n",
            "[134 | 328.90] loss=2.72 avg=2.90\n",
            "[135 | 331.22] loss=2.87 avg=2.90\n",
            "[136 | 333.54] loss=2.73 avg=2.90\n",
            "[137 | 335.85] loss=2.67 avg=2.89\n",
            "[138 | 338.17] loss=2.78 avg=2.89\n",
            "[139 | 340.48] loss=2.80 avg=2.89\n",
            "[140 | 342.81] loss=2.72 avg=2.89\n",
            "[141 | 345.13] loss=2.80 avg=2.89\n",
            "[142 | 347.44] loss=2.69 avg=2.88\n",
            "[143 | 349.75] loss=2.82 avg=2.88\n",
            "[144 | 352.07] loss=2.72 avg=2.88\n",
            "[145 | 354.39] loss=2.67 avg=2.88\n",
            "[146 | 356.70] loss=2.56 avg=2.87\n",
            "[147 | 359.02] loss=2.68 avg=2.87\n",
            "[148 | 361.33] loss=2.69 avg=2.87\n",
            "[149 | 363.64] loss=2.69 avg=2.87\n",
            "[150 | 365.96] loss=2.58 avg=2.86\n",
            "[151 | 368.27] loss=2.68 avg=2.86\n",
            "[152 | 370.58] loss=2.67 avg=2.86\n",
            "[153 | 372.90] loss=2.62 avg=2.86\n",
            "[154 | 375.21] loss=2.69 avg=2.85\n",
            "[155 | 377.53] loss=2.76 avg=2.85\n",
            "[156 | 379.85] loss=2.70 avg=2.85\n",
            "[157 | 382.16] loss=2.59 avg=2.85\n",
            "[158 | 384.48] loss=2.71 avg=2.85\n",
            "[159 | 386.78] loss=2.82 avg=2.85\n",
            "[160 | 389.10] loss=2.64 avg=2.84\n",
            "[161 | 391.42] loss=2.54 avg=2.84\n",
            "[162 | 393.74] loss=2.73 avg=2.84\n",
            "[163 | 396.05] loss=2.69 avg=2.84\n",
            "[164 | 398.37] loss=2.74 avg=2.83\n",
            "[165 | 400.69] loss=2.63 avg=2.83\n",
            "[166 | 403.00] loss=2.79 avg=2.83\n",
            "[167 | 405.32] loss=2.84 avg=2.83\n",
            "[168 | 407.63] loss=2.65 avg=2.83\n",
            "[169 | 409.95] loss=2.82 avg=2.83\n",
            "[170 | 412.26] loss=2.44 avg=2.82\n",
            "[171 | 414.58] loss=2.69 avg=2.82\n",
            "[172 | 416.90] loss=2.68 avg=2.82\n",
            "[173 | 419.21] loss=2.67 avg=2.82\n",
            "[174 | 421.53] loss=2.61 avg=2.82\n",
            "[175 | 423.85] loss=2.60 avg=2.81\n",
            "[176 | 426.17] loss=2.51 avg=2.81\n",
            "[177 | 428.49] loss=2.68 avg=2.81\n",
            "[178 | 430.80] loss=2.68 avg=2.81\n",
            "[179 | 433.11] loss=2.61 avg=2.81\n",
            "[180 | 435.43] loss=2.67 avg=2.80\n",
            "[181 | 437.73] loss=2.69 avg=2.80\n",
            "[182 | 440.05] loss=2.56 avg=2.80\n",
            "[183 | 442.37] loss=2.78 avg=2.80\n",
            "[184 | 444.68] loss=2.50 avg=2.80\n",
            "[185 | 447.00] loss=2.81 avg=2.80\n",
            "[186 | 449.31] loss=2.51 avg=2.79\n",
            "[187 | 451.64] loss=2.44 avg=2.79\n",
            "[188 | 453.95] loss=2.73 avg=2.79\n",
            "[189 | 456.26] loss=2.52 avg=2.78\n",
            "[190 | 458.57] loss=2.70 avg=2.78\n",
            "[191 | 460.88] loss=2.61 avg=2.78\n",
            "[192 | 463.20] loss=2.46 avg=2.78\n",
            "[193 | 465.52] loss=2.49 avg=2.77\n",
            "[194 | 467.84] loss=2.56 avg=2.77\n",
            "[195 | 470.16] loss=2.58 avg=2.77\n",
            "[196 | 472.47] loss=2.66 avg=2.77\n",
            "[197 | 474.79] loss=2.61 avg=2.77\n",
            "[198 | 477.11] loss=2.65 avg=2.77\n",
            "[199 | 479.42] loss=2.56 avg=2.76\n",
            "[200 | 481.73] loss=2.51 avg=2.76\n",
            "======== SAMPLE 1 ========\n",
            " Winter, and the man said, 'What should I be doing when my father wakes?\" \"Your father has no need of me, my lady,\" Bronn said. \"He's the Lord of Riverrun. You can stay home.\" The boy's lips curled in a frown. If only he could sit the other nights, he thought. He'd have to stay. Perhaps even go to bed with Bronn. The dream was too much. \"You have an \n",
            "admirer. I was thinking he was a shadow, I could see him sometimes. He had a beard and a big \n",
            "helm. I was thinking he lived to be a thousand years old. He was always \n",
            "wondering what the old men were like. When I went up there, I saw them with my own eyes. I looked at \n",
            "Bronn and said, 'The Lord of Winter is a Shadow,' and I had no notion whether he lived to be a thousand \n",
            "years old or not. He told me the faces and the faces of lords and knights. They all stood against \n",
            "the Wall, and my uncle Ser Rodrik used to tell me that the old men were always looking for the Old Ones. \n",
            "\"My uncle Ser Rodrik used to say that as well,\" Bronn said, \"but I told him that there were only three.\" \n",
            "Page 468\n",
            "\n",
            "A new dawn was looming ahead. It did look like rain, brooding across the sky, with black clouds drifting \n",
            "between them. \n",
            "\"A man with a shadow,\" Ser Rodrik said to the boy. \"A shadow . . .\" \n",
            "A dream was making its way through the boy's head. His mind was spinning, his hair stirring. \"A dream. \n",
            "The sound was all the same, as if the gods had found the right man to give his name to. \n",
            "Ser Rodrik smiled. \"Tomorrow, no matter what happens, my lady, tomorrow we'll be fighting in the King Street \n",
            "battle.\" A strange, mocking smile. \n",
            "\"Are you sure you want to fight?\" Bronn said to Ser Rodrik. \n",
            "This time they thought it a man's right. The next day Bronn went to bed with Ser Rodrik, talking to Ser Rodrik, \n",
            "saying, \"I shall be arriving in Nyx the day this war comes to an end . . .\" \n",
            "Ser Rodrik turned to them and said, \"Ser Rodrik, I have no wish to fight.\" The children were all laughing in Dany's \n",
            "face. \n",
            "When they woke in the tent, the children were all staring at the shadow of the great wooden sword it had come \n",
            "from. \"Come and join us,\" Bronn told them. Ser Rodrik had called. \n",
            "\"You are welcome,\" Bronn told them to come and say, \"and be glad that I do not share your burdens with \n",
            "my brother Lord Tywin,\" and Ser Rodrik followed suit. \n",
            "Ser Rodrik saw them for the first time. The shadow of victory was already in the trees. Bronn made a \n",
            "hairbrush down over them, and the children went silent, yet they knew him. That day, if the war came to an end \n",
            "the rest of the war will follow \n",
            "Lord Tywin like a shadow. \n",
            "Page 469\n",
            "\n",
            "He would be gone in a fortnight. Perhaps a week. Tyrion could command the household and sell \n",
            "them horse for silver, and the Lord would sell them gold for their silver. The Lord of Light would take them and \n",
            "take them everywhere. \"You can expect a hundred swords, Ser Rodrik,\" Bronn said in a low voice. \n",
            "\"No,\" Tyrion said. \"You are welcome here, and a thousand silver, as long as you keep to Lord Tywin's \n",
            "wastes.\" \n",
            "Ser Rodrik would not say how many they would sell Tyrion. He seemed to take it on faith, even before they reached the \n",
            "king's commandpost. Tyrion Lannister was a stranger to them than he was to Tyrion Stark; in truth he had no \n",
            "interest at all. The children had been taken to bed with Ser Rodrik's sword in hand, and he seemed utterly \n",
            "disappointed with their absence. The king's armorer was a woman's maid, he would have you believe, and he \n",
            "feared the children would be able to keep his wife and keep his children. \n",
            "It was not Ser Rodrik's wish to remain here forever, yet it was the wish of them all. A thousand \n",
            "men and a thousand beasts in his yard. Tyrion had no place here. Tyrion Lannister would have us all, if the old \n",
            "camel had taught him the ways of the beast. \"Take these children away,\" he ordered the steward. The children \n",
            "\n",
            "[201 | 495.20] loss=2.52 avg=2.76\n",
            "[202 | 497.51] loss=2.45 avg=2.75\n",
            "[203 | 499.83] loss=2.70 avg=2.75\n",
            "[204 | 502.14] loss=2.57 avg=2.75\n",
            "[205 | 504.46] loss=2.62 avg=2.75\n",
            "[206 | 506.77] loss=2.56 avg=2.75\n",
            "[207 | 509.08] loss=2.71 avg=2.75\n",
            "[208 | 511.40] loss=2.61 avg=2.75\n",
            "[209 | 513.72] loss=2.55 avg=2.74\n",
            "[210 | 516.03] loss=2.73 avg=2.74\n",
            "[211 | 518.34] loss=2.57 avg=2.74\n",
            "[212 | 520.65] loss=2.50 avg=2.74\n",
            "[213 | 522.98] loss=2.64 avg=2.74\n",
            "[214 | 525.29] loss=2.67 avg=2.74\n",
            "[215 | 527.60] loss=2.53 avg=2.73\n",
            "[216 | 529.92] loss=2.60 avg=2.73\n",
            "[217 | 532.23] loss=2.63 avg=2.73\n",
            "[218 | 534.56] loss=2.55 avg=2.73\n",
            "[219 | 536.87] loss=2.49 avg=2.73\n",
            "[220 | 539.18] loss=2.63 avg=2.73\n",
            "[221 | 541.50] loss=2.38 avg=2.72\n",
            "[222 | 543.80] loss=2.42 avg=2.72\n",
            "[223 | 546.12] loss=2.69 avg=2.72\n",
            "[224 | 548.45] loss=2.40 avg=2.71\n",
            "[225 | 550.76] loss=2.62 avg=2.71\n",
            "[226 | 553.08] loss=2.55 avg=2.71\n",
            "[227 | 555.39] loss=2.38 avg=2.71\n",
            "[228 | 557.70] loss=2.51 avg=2.71\n",
            "[229 | 560.02] loss=2.46 avg=2.70\n",
            "[230 | 562.34] loss=2.55 avg=2.70\n",
            "[231 | 564.66] loss=2.49 avg=2.70\n",
            "[232 | 566.97] loss=2.51 avg=2.70\n",
            "[233 | 569.28] loss=2.65 avg=2.70\n",
            "[234 | 571.60] loss=2.50 avg=2.69\n",
            "[235 | 573.91] loss=2.35 avg=2.69\n",
            "[236 | 576.23] loss=2.55 avg=2.69\n",
            "[237 | 578.55] loss=2.43 avg=2.69\n",
            "[238 | 580.86] loss=2.62 avg=2.69\n",
            "[239 | 583.17] loss=2.32 avg=2.68\n",
            "[240 | 585.50] loss=2.56 avg=2.68\n",
            "[241 | 587.82] loss=2.35 avg=2.68\n",
            "[242 | 590.14] loss=2.46 avg=2.67\n",
            "[243 | 592.45] loss=2.58 avg=2.67\n",
            "[244 | 594.76] loss=2.54 avg=2.67\n",
            "[245 | 597.09] loss=2.41 avg=2.67\n",
            "[246 | 599.41] loss=2.34 avg=2.66\n",
            "[247 | 601.72] loss=2.30 avg=2.66\n",
            "[248 | 604.04] loss=2.38 avg=2.66\n",
            "[249 | 606.35] loss=2.48 avg=2.66\n",
            "[250 | 608.66] loss=2.57 avg=2.66\n",
            "[251 | 610.98] loss=2.48 avg=2.65\n",
            "[252 | 613.30] loss=2.46 avg=2.65\n",
            "[253 | 615.61] loss=2.47 avg=2.65\n",
            "[254 | 617.93] loss=2.30 avg=2.65\n",
            "[255 | 620.24] loss=2.37 avg=2.64\n",
            "[256 | 622.57] loss=2.51 avg=2.64\n",
            "[257 | 624.88] loss=2.46 avg=2.64\n",
            "[258 | 627.19] loss=2.36 avg=2.64\n",
            "[259 | 629.51] loss=2.46 avg=2.63\n",
            "[260 | 631.82] loss=2.40 avg=2.63\n",
            "[261 | 634.14] loss=2.13 avg=2.63\n",
            "[262 | 636.46] loss=2.36 avg=2.62\n",
            "[263 | 638.77] loss=2.20 avg=2.62\n",
            "[264 | 641.08] loss=2.65 avg=2.62\n",
            "[265 | 643.39] loss=2.37 avg=2.62\n",
            "[266 | 645.71] loss=2.46 avg=2.61\n",
            "[267 | 648.02] loss=2.52 avg=2.61\n",
            "[268 | 650.34] loss=2.22 avg=2.61\n",
            "[269 | 652.65] loss=2.39 avg=2.61\n",
            "[270 | 654.96] loss=2.51 avg=2.61\n",
            "[271 | 657.29] loss=2.31 avg=2.60\n",
            "[272 | 659.61] loss=2.39 avg=2.60\n",
            "[273 | 661.92] loss=2.40 avg=2.60\n",
            "[274 | 664.23] loss=2.30 avg=2.60\n",
            "[275 | 666.54] loss=2.22 avg=2.59\n",
            "[276 | 668.86] loss=2.44 avg=2.59\n",
            "[277 | 671.18] loss=2.27 avg=2.59\n",
            "[278 | 673.48] loss=2.75 avg=2.59\n",
            "[279 | 675.80] loss=2.27 avg=2.58\n",
            "[280 | 678.10] loss=2.39 avg=2.58\n",
            "[281 | 680.42] loss=2.20 avg=2.58\n",
            "[282 | 682.75] loss=2.56 avg=2.58\n",
            "[283 | 685.06] loss=2.74 avg=2.58\n",
            "[284 | 687.37] loss=2.27 avg=2.58\n",
            "[285 | 689.69] loss=2.24 avg=2.57\n",
            "[286 | 692.01] loss=2.63 avg=2.57\n",
            "[287 | 694.32] loss=2.23 avg=2.57\n",
            "[288 | 696.64] loss=2.31 avg=2.57\n",
            "[289 | 698.95] loss=2.27 avg=2.56\n",
            "[290 | 701.27] loss=2.33 avg=2.56\n",
            "[291 | 703.59] loss=2.30 avg=2.56\n",
            "[292 | 705.91] loss=2.54 avg=2.56\n",
            "[293 | 708.23] loss=2.30 avg=2.56\n",
            "[294 | 710.54] loss=2.34 avg=2.55\n",
            "[295 | 712.86] loss=2.45 avg=2.55\n",
            "[296 | 715.17] loss=2.36 avg=2.55\n",
            "[297 | 717.48] loss=2.15 avg=2.55\n",
            "[298 | 719.80] loss=2.20 avg=2.54\n",
            "[299 | 722.11] loss=2.38 avg=2.54\n",
            "[300 | 724.42] loss=2.42 avg=2.54\n",
            "======== SAMPLE 1 ========\n",
            " any for her, but her father had never spoken a word of it to him. Arya was always suspicious of Stark and suspicious of her uncle, and yet she could not say that Stark had spoken a single word of the truth of Lysa's death, and she scarcely dared to put a finger on who she was. There were songs in the Mirror that said the Starks drank Stark orange, and songs about being a Lannister were sung by men who looked like Stark twins, and she did not recall ever hearing of a man who called himself an Englishman, no matter how many years he had been in this life. Sansa was so frightened by her eyes that she dreamed all her dreams of Robb riding north, and when she woke, her father would never have asked one question to her, and none of it was asked to her. When she had to go to the crypts, she would have to be sure that Arya was safe before they threw her into the river, so she had to fear. When she was older, she thought it was better to be dead than buried . . . and she was not so sure. If I die . . . well, she was fourteen. It was said that Jon won the Eyrie at the Summer Fair, by the way, and after he left Winterfell he gave it up to Jory, who loved him so much that he made him his bedding, and Bran was nine, and all his friends were dead and gone and would not talk to him. She was fifteen now. Arya was twenty. Her father told her that Jory always talked of her as if she were the last of his kind, that she would never be the same, that she would be as ugly as the ones she loved. So now she loved Jory. \"You do not hear me that,\" she said to her mother. \"You must never hear me that, for the love you bear me, that is the last thing I would have wanted, if someone had been hurt by someone . . .\" \n",
            "\"Lord Stannis, don't talk, Sansa,\" Jory urged her father. \"Not even from me. No one says the words. I did it myself. You can't talk to me alone, you've had your chance with the queen, I'll teach you them, there is nothing wrong with that, but it wouldn't be right, you have to die for your queen, and there is nothing wrong with that either, so you die for her. Do you see that? What you are is no Stark girl, there is nothing wrong with that. You did it yourself, and as you will ever see, it's not right, you've had a chance and it's not the way, so why not? My little sister did it herself too, and damn it, she did too. So why should I die for her? She's the queen and all she's done is done what she wants. Let her be queen. We will all be so much alike. Do you see that, Father? It's all right, I'll be the first woman to the Iron Throne, that's all it is. And I'll be queen, I'll be the first, and you'll be the last, and you'll be the one that dies . . .\" \n",
            "\"I'm the one die for her!\" \n",
            "Catelyn had the power to see that, and so she spoke, and the child was born. \n",
            "She had found the way, she told herself, but when she turned back, a great deep red glow hung over the dark iron door behind them. \n",
            "Page 515\n",
            "\n",
            "The dwarf stepped out and pointed a finger at her. \"Arya, don't tell her you're pregnant, Father, don't tell her it's a fever, don't tell her it's your son, tell her, tell her, don't tell her, it's not the girl, don't tell her, do it!\" \n",
            "\"I have had chances, she said. Why not?\" Sansa stood. \"Your brother is with me now, Jon's with me, he'll be back soon, Robb's with me, I'll be with him when he's finished, I'll have a look at his name day, he'll see to it, and when he's done, \n",
            "I'll be sure to name him Arya.\" She took her baby to her, and it looked so baby. She hugged her little baby hard and kissed him fiercely. She could hear the pain, the cold of her \n",
            "heart. \n",
            "Jon had come first, after she'd called him, and Sansa had taken over. Arya had taken over the reins, and when she was \n",
            "done she was gone. She was gone as well, though; she was a blank, like her sister. In her little bubble she went. And now she had not looked at her \n",
            "children, not even to Jon. Only to them\n",
            "\n",
            "[301 | 737.91] loss=2.14 avg=2.54\n",
            "[302 | 740.22] loss=2.26 avg=2.53\n",
            "[303 | 742.54] loss=2.42 avg=2.53\n",
            "[304 | 744.86] loss=2.34 avg=2.53\n",
            "[305 | 747.17] loss=2.36 avg=2.53\n",
            "[306 | 749.49] loss=2.17 avg=2.52\n",
            "[307 | 751.80] loss=2.20 avg=2.52\n",
            "[308 | 754.11] loss=2.37 avg=2.52\n",
            "[309 | 756.43] loss=2.16 avg=2.51\n",
            "[310 | 758.74] loss=2.43 avg=2.51\n",
            "[311 | 761.06] loss=2.15 avg=2.51\n",
            "[312 | 763.38] loss=2.26 avg=2.51\n",
            "[313 | 765.70] loss=2.19 avg=2.50\n",
            "[314 | 768.02] loss=2.60 avg=2.51\n",
            "[315 | 770.33] loss=2.19 avg=2.50\n",
            "[316 | 772.65] loss=2.27 avg=2.50\n",
            "[317 | 774.96] loss=2.22 avg=2.50\n",
            "[318 | 777.27] loss=2.18 avg=2.49\n",
            "[319 | 779.59] loss=2.39 avg=2.49\n",
            "[320 | 781.91] loss=2.16 avg=2.49\n",
            "[321 | 784.23] loss=2.24 avg=2.49\n",
            "[322 | 786.54] loss=2.31 avg=2.48\n",
            "[323 | 788.86] loss=2.31 avg=2.48\n",
            "[324 | 791.18] loss=2.58 avg=2.48\n",
            "[325 | 793.48] loss=2.38 avg=2.48\n",
            "[326 | 795.79] loss=2.20 avg=2.48\n",
            "[327 | 798.10] loss=2.41 avg=2.48\n",
            "[328 | 800.42] loss=2.33 avg=2.48\n",
            "[329 | 802.73] loss=2.11 avg=2.47\n",
            "[330 | 805.05] loss=2.17 avg=2.47\n",
            "[331 | 807.37] loss=2.17 avg=2.47\n",
            "[332 | 809.69] loss=2.37 avg=2.47\n",
            "[333 | 812.00] loss=2.11 avg=2.46\n",
            "[334 | 814.32] loss=2.41 avg=2.46\n",
            "[335 | 816.63] loss=2.31 avg=2.46\n",
            "[336 | 818.94] loss=2.30 avg=2.46\n",
            "[337 | 821.26] loss=2.07 avg=2.45\n",
            "[338 | 823.57] loss=2.25 avg=2.45\n",
            "[339 | 825.89] loss=2.27 avg=2.45\n",
            "[340 | 828.22] loss=2.30 avg=2.45\n",
            "[341 | 830.53] loss=1.92 avg=2.44\n",
            "[342 | 832.84] loss=2.31 avg=2.44\n",
            "[343 | 835.14] loss=2.05 avg=2.44\n",
            "[344 | 837.46] loss=2.26 avg=2.44\n",
            "[345 | 839.77] loss=2.42 avg=2.44\n",
            "[346 | 842.08] loss=2.12 avg=2.43\n",
            "[347 | 844.40] loss=2.15 avg=2.43\n",
            "[348 | 846.71] loss=2.35 avg=2.43\n",
            "[349 | 849.03] loss=1.97 avg=2.42\n",
            "[350 | 851.35] loss=2.22 avg=2.42\n",
            "[351 | 853.67] loss=2.46 avg=2.42\n",
            "[352 | 855.98] loss=2.12 avg=2.42\n",
            "[353 | 858.29] loss=2.38 avg=2.42\n",
            "[354 | 860.61] loss=2.18 avg=2.42\n",
            "[355 | 862.93] loss=2.29 avg=2.42\n",
            "[356 | 865.24] loss=2.47 avg=2.42\n",
            "[357 | 867.55] loss=2.14 avg=2.41\n",
            "[358 | 869.86] loss=2.34 avg=2.41\n",
            "[359 | 872.18] loss=1.96 avg=2.41\n",
            "[360 | 874.49] loss=2.22 avg=2.41\n",
            "[361 | 876.81] loss=2.28 avg=2.40\n",
            "[362 | 879.13] loss=2.10 avg=2.40\n",
            "[363 | 881.44] loss=2.32 avg=2.40\n",
            "[364 | 883.75] loss=2.20 avg=2.40\n",
            "[365 | 886.07] loss=2.12 avg=2.40\n",
            "[366 | 888.39] loss=2.30 avg=2.39\n",
            "[367 | 890.71] loss=2.16 avg=2.39\n",
            "[368 | 893.02] loss=2.17 avg=2.39\n",
            "[369 | 895.34] loss=2.03 avg=2.39\n",
            "[370 | 897.65] loss=2.03 avg=2.38\n",
            "[371 | 899.97] loss=2.51 avg=2.38\n",
            "[372 | 902.29] loss=2.16 avg=2.38\n",
            "[373 | 904.62] loss=2.22 avg=2.38\n",
            "[374 | 906.93] loss=2.03 avg=2.38\n",
            "[375 | 909.24] loss=2.31 avg=2.38\n",
            "[376 | 911.56] loss=2.02 avg=2.37\n",
            "[377 | 913.87] loss=1.95 avg=2.37\n",
            "[378 | 916.19] loss=2.25 avg=2.37\n",
            "[379 | 918.51] loss=1.96 avg=2.36\n",
            "[380 | 920.82] loss=1.95 avg=2.36\n",
            "[381 | 923.14] loss=2.13 avg=2.36\n",
            "[382 | 925.46] loss=1.97 avg=2.35\n",
            "[383 | 927.78] loss=2.24 avg=2.35\n",
            "[384 | 930.10] loss=2.03 avg=2.35\n",
            "[385 | 932.42] loss=2.33 avg=2.35\n",
            "[386 | 934.73] loss=2.09 avg=2.35\n",
            "[387 | 937.05] loss=2.09 avg=2.34\n",
            "[388 | 939.37] loss=1.84 avg=2.34\n",
            "[389 | 941.68] loss=2.10 avg=2.33\n",
            "[390 | 944.00] loss=2.02 avg=2.33\n",
            "[391 | 946.31] loss=2.11 avg=2.33\n",
            "[392 | 948.64] loss=2.24 avg=2.33\n",
            "[393 | 950.95] loss=1.99 avg=2.33\n",
            "[394 | 953.26] loss=2.23 avg=2.32\n",
            "[395 | 955.58] loss=2.02 avg=2.32\n",
            "[396 | 957.89] loss=2.08 avg=2.32\n",
            "[397 | 960.21] loss=1.80 avg=2.31\n",
            "[398 | 962.53] loss=2.34 avg=2.31\n",
            "[399 | 964.85] loss=1.90 avg=2.31\n",
            "[400 | 967.16] loss=2.16 avg=2.31\n",
            "======== SAMPLE 1 ========\n",
            "'s her fault, and only herself \n",
            "would have the courage to think she would allow the boy to die anyway, let alone to begin a life of his own. \n",
            "She had seen it, seen the grief in the boy's eyes. She had tried so hard, yet she could not \n",
            "have it both ways. She had tried to bear the boy, she knew it, her prayers were answered, but her \n",
            "life's pain was running through her now. She had told Jeyne once, she'd hoped, but she could not \n",
            "know for certain what would have happened if Ser Jaime had won the duel. She was sorry for him \n",
            "now, but her heart had still never forgiven herself. \n",
            "They shared a bed. Jeyne did not sleep so much as sleep with Ser Rodrik. It was said that after every \n",
            "fight he would go to Ser Glover, who had been his father's dearest friend. His death would \n",
            "lead to another quarrel rather than more, and his brother would take the king's place. That was the worst \n",
            "way. Even after his grief had settled over them, they had remained friends. \n",
            "She did not think they ever talked of love ever since. Jyckon and Jeyne looked at each other as if \n",
            "its secrets were secrets. The grief was so raw in her, yet Sansa never dared speak of it. It \n",
            "was not until she was alone with Prince Joffrey before her bed shift that Jeyne would send \n",
            "me out to find her. Wept so hard she could barely bear it. She took solace in her dreams. \n",
            "The boy was black with grief, but not in hers. \"This must be Ben Stark's castle,\" she \n",
            "told him one night when they were alone. \"No one can ride a horse without falling.\" \n",
            "Jeyne's smile was a pale reflection of hers, but she had not thought to smile after that. She looked at all the \n",
            "men around them, and thought, There are better ways to get around. The way was almost deserted. \n",
            "Sandor Clegane had sent for the captain of the guard, but he was still too old to come near the bridge. He \n",
            "would have to wait until his men came back with their horses, as was his duty. He would have to ride another day. \n",
            "\"Do as the man commands,\" Jyckon growled. It might have been best had he put his feet in hers, or at least \n",
            "his arms if they had not been fastened around his shoulders. Her stomach turned itself around. For a moment she was \n",
            "as angry as she had ever been. Yet then someone said, \"Stay safe, boy.\" \n",
            "Jeyne rose, grabbed by the blankets. The guardsman was all alone as well. Her stomach turned. She grabbed him \n",
            "holdfully, but he pulled her down, held her tight. A few moments later Ser Robar made for his galley, \n",
            "and the others were gathering in the yard, with Jeyne's father behind them. No one \n",
            "ever came forward to confront them. It was all too much, the look on Jyckon's face, the way they whispered \n",
            "what they had seen; she thought they would kill him, or worse, he'd kill the women. \n",
            "She took comfort from that, though. She knew they never said what they saw, that they seldom \n",
            "heard, never saw how far away they were. Even now, she could hear their laughter, the deep mumbling about \n",
            "the bird that was Ser Marq Piper. The birds had no mind to call them birds, they were all laughing. The truth \n",
            "of what had happened was far more pressing. \n",
            "\"Are you all right?\" one of them asked. \n",
            "\"I am half right.\" Jeyne looked at all her men, some with brown hair and scarred faces, some as broad \n",
            "as a foal's shoulder. All they saw was laughter, and then silence as they turned to run. They ran \n",
            "for their lives, down the mooring. When they turned back, the girl behind them was screaming. She was sobbing \n",
            "in pain, crying all the while. \n",
            "When the last of them were gone, a thick grey noise seemed to gather around them. Her father \n",
            "suddenly broke the silence, swept her hair back, and began to shout. \"The stag is coming!\" \n",
            "It was just as terrifying as climbing the walls and following the scent of the animal, she knew. She was afraid for her \n",
            "foe, she knew. They would see, they would hear, but it was all too much; for a long while nothing moved \n",
            "and they were all alone. They would cry, watch, tear, grief, shame. Then there would\n",
            "\n",
            "[401 | 980.59] loss=1.93 avg=2.30\n",
            "[402 | 982.91] loss=2.19 avg=2.30\n",
            "[403 | 985.23] loss=2.01 avg=2.30\n",
            "[404 | 987.54] loss=2.11 avg=2.30\n",
            "[405 | 989.86] loss=2.25 avg=2.30\n",
            "[406 | 992.18] loss=2.12 avg=2.30\n",
            "[407 | 994.49] loss=2.03 avg=2.29\n",
            "[408 | 996.81] loss=1.96 avg=2.29\n",
            "[409 | 999.13] loss=1.91 avg=2.29\n",
            "[410 | 1001.44] loss=2.18 avg=2.28\n",
            "[411 | 1003.75] loss=1.88 avg=2.28\n",
            "[412 | 1006.07] loss=2.14 avg=2.28\n",
            "[413 | 1008.39] loss=1.90 avg=2.28\n",
            "[414 | 1010.71] loss=2.06 avg=2.27\n",
            "[415 | 1013.02] loss=1.97 avg=2.27\n",
            "[416 | 1015.34] loss=1.88 avg=2.27\n",
            "[417 | 1017.65] loss=1.93 avg=2.26\n",
            "[418 | 1019.96] loss=2.09 avg=2.26\n",
            "[419 | 1022.28] loss=1.73 avg=2.26\n",
            "[420 | 1024.59] loss=2.12 avg=2.25\n",
            "[421 | 1026.91] loss=2.05 avg=2.25\n",
            "[422 | 1029.23] loss=2.01 avg=2.25\n",
            "[423 | 1031.55] loss=2.10 avg=2.25\n",
            "[424 | 1033.88] loss=1.82 avg=2.24\n",
            "[425 | 1036.19] loss=1.84 avg=2.24\n",
            "[426 | 1038.50] loss=1.85 avg=2.24\n",
            "[427 | 1040.82] loss=1.85 avg=2.23\n",
            "[428 | 1043.13] loss=1.82 avg=2.23\n",
            "[429 | 1045.46] loss=1.85 avg=2.22\n",
            "[430 | 1047.77] loss=2.07 avg=2.22\n",
            "[431 | 1050.08] loss=2.00 avg=2.22\n",
            "[432 | 1052.39] loss=1.80 avg=2.22\n",
            "[433 | 1054.70] loss=1.95 avg=2.21\n",
            "[434 | 1057.01] loss=1.99 avg=2.21\n",
            "[435 | 1059.33] loss=1.81 avg=2.21\n",
            "[436 | 1061.64] loss=1.59 avg=2.20\n",
            "[437 | 1063.96] loss=1.72 avg=2.20\n",
            "[438 | 1066.27] loss=1.86 avg=2.19\n",
            "[439 | 1068.58] loss=1.96 avg=2.19\n",
            "[440 | 1070.91] loss=1.89 avg=2.19\n",
            "[441 | 1073.22] loss=1.91 avg=2.18\n",
            "[442 | 1075.53] loss=2.19 avg=2.18\n",
            "[443 | 1077.83] loss=1.87 avg=2.18\n",
            "[444 | 1080.15] loss=2.02 avg=2.18\n",
            "[445 | 1082.46] loss=1.85 avg=2.18\n",
            "[446 | 1084.77] loss=2.24 avg=2.18\n",
            "[447 | 1087.09] loss=1.88 avg=2.17\n",
            "[448 | 1089.40] loss=2.27 avg=2.17\n",
            "[449 | 1091.72] loss=1.94 avg=2.17\n",
            "[450 | 1094.05] loss=1.77 avg=2.17\n",
            "[451 | 1096.35] loss=2.19 avg=2.17\n",
            "[452 | 1098.66] loss=2.02 avg=2.17\n",
            "[453 | 1100.97] loss=1.68 avg=2.16\n",
            "[454 | 1103.28] loss=1.74 avg=2.16\n",
            "[455 | 1105.60] loss=1.57 avg=2.15\n",
            "[456 | 1107.91] loss=1.73 avg=2.15\n",
            "[457 | 1110.22] loss=1.73 avg=2.14\n",
            "[458 | 1112.54] loss=1.91 avg=2.14\n",
            "[459 | 1114.86] loss=1.94 avg=2.14\n",
            "[460 | 1117.18] loss=2.17 avg=2.14\n",
            "[461 | 1119.49] loss=1.81 avg=2.14\n",
            "[462 | 1121.80] loss=2.11 avg=2.14\n",
            "[463 | 1124.11] loss=2.08 avg=2.14\n",
            "[464 | 1126.43] loss=1.85 avg=2.13\n",
            "[465 | 1128.75] loss=2.01 avg=2.13\n",
            "[466 | 1131.07] loss=2.11 avg=2.13\n",
            "[467 | 1133.39] loss=2.02 avg=2.13\n",
            "[468 | 1135.70] loss=2.28 avg=2.13\n",
            "[469 | 1138.02] loss=1.98 avg=2.13\n",
            "[470 | 1140.33] loss=1.80 avg=2.13\n",
            "[471 | 1142.66] loss=2.07 avg=2.13\n",
            "[472 | 1144.97] loss=1.80 avg=2.12\n",
            "[473 | 1147.29] loss=2.02 avg=2.12\n",
            "[474 | 1149.61] loss=1.56 avg=2.12\n",
            "[475 | 1151.93] loss=2.33 avg=2.12\n",
            "[476 | 1154.25] loss=1.72 avg=2.11\n",
            "[477 | 1156.57] loss=2.04 avg=2.11\n",
            "[478 | 1158.88] loss=1.94 avg=2.11\n",
            "[479 | 1161.20] loss=1.85 avg=2.11\n",
            "[480 | 1163.52] loss=1.79 avg=2.11\n",
            "[481 | 1165.84] loss=1.78 avg=2.10\n",
            "[482 | 1168.16] loss=1.85 avg=2.10\n",
            "[483 | 1170.48] loss=2.02 avg=2.10\n",
            "[484 | 1172.80] loss=1.53 avg=2.09\n",
            "[485 | 1175.12] loss=1.83 avg=2.09\n",
            "[486 | 1177.45] loss=1.81 avg=2.09\n",
            "[487 | 1179.78] loss=2.06 avg=2.09\n",
            "[488 | 1182.10] loss=1.53 avg=2.08\n",
            "[489 | 1184.42] loss=1.63 avg=2.08\n",
            "[490 | 1186.73] loss=1.80 avg=2.07\n",
            "[491 | 1189.05] loss=1.99 avg=2.07\n",
            "[492 | 1191.36] loss=2.08 avg=2.07\n",
            "[493 | 1193.68] loss=1.77 avg=2.07\n",
            "[494 | 1196.00] loss=1.48 avg=2.06\n",
            "[495 | 1198.32] loss=1.58 avg=2.06\n",
            "[496 | 1200.63] loss=1.65 avg=2.06\n",
            "[497 | 1202.95] loss=1.64 avg=2.05\n",
            "[498 | 1205.27] loss=1.97 avg=2.05\n",
            "[499 | 1207.58] loss=1.80 avg=2.05\n",
            "[500 | 1209.90] loss=1.54 avg=2.04\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Present\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac7cbbee-a546-4ace-f630-ce76501395fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Present that he's too old for \n",
            "the dosh khaleen, and too young for the khal, and too bold for the \n",
            "guard. Better he'll get us to King's Landing and offer us his hospitality. I'm not sure I'll \n",
            "ever forget that.\" The guardsman turned and stared at her, a pale reflection of a boy all in his thirties, \n",
            "yet he knew she was dreaming. \"You have a pretty face, Maester. A gentlewoman.\" \n",
            "Page 186\n",
            "\n",
            "\"I see,\" Maester Luwin said, not so much to Jon's surprise, but to Maege's dismay too. \"And you like \n",
            "your hair.\" \n",
            "\"Lately\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "661d85b7-82dc-4dd5-909e-1efab2e33e39"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "id": "g7K9X3K8TEwj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}